#!/usr/bin/env python3
"""
Multi-GPU Run Script

å¿«é€ŸæŒ‡å®šå¤šGPUå¹¶è¿è¡ŒRL-Probeæµç¨‹çš„ä¾¿æ·è„šæœ¬ã€‚

ç”¨æ³•ç¤ºä¾‹:
    # ä½¿ç”¨GPU 0,1,2
    python run_multi_gpu.py --gpus 0,4,5

    # ä½¿ç”¨GPU 0,1,2,3ï¼Œæ¯ä¸ªGPUé™åˆ¶30GBæ˜¾å­˜
    python run_multi_gpu.py --gpus 4,5 --max-memory 40GB

    # ä½¿ç”¨æ‰€æœ‰å¯ç”¨GPUï¼Œè‡ªåŠ¨å‡è¡¡æ¨¡å¼
    python run_multi_gpu.py --gpus all --device auto

    # åªè¿è¡Œç‰¹å®šæ­¥éª¤
    python run_multi_gpu.py --gpus 0,1 --steps prepare,rollout

    # å¹²è¿è¡Œæ¨¡å¼ï¼ˆåªæ˜¾ç¤ºé…ç½®ä¸å®é™…è¿è¡Œï¼‰
    python run_multi_gpu.py --gpus 0,1,2 --dry-run
"""

import argparse
import logging
import os
import subprocess
import sys
from datetime import datetime
from pathlib import Path
from typing import List, Optional

import torch
import yaml

def setup_logging(log_dir: str = None):
    """è®¾ç½®æ—¥å¿—è®°å½•åˆ°æ–‡ä»¶å’Œæ§åˆ¶å°"""
    if log_dir:
        log_dir = Path(log_dir)
        log_dir.mkdir(parents=True, exist_ok=True)
        log_file = log_dir / f"run_multi_gpu_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    else:
        log_file = None

    logger = logging.getLogger(__name__)
    logger.setLevel(logging.INFO)

    # æ§åˆ¶å°å¤„ç†å™¨
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_formatter = logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")
    console_handler.setFormatter(console_formatter)
    logger.addHandler(console_handler)

    # æ–‡ä»¶å¤„ç†å™¨
    if log_file:
        file_handler = logging.FileHandler(log_file)
        file_handler.setLevel(logging.INFO)
        file_formatter = logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")
        file_handler.setFormatter(file_formatter)
        logger.addHandler(file_handler)
        logger.info(f"æ—¥å¿—ä¿å­˜è‡³: {log_file}")

    return logger, log_file

logger, log_file = setup_logging()


def parse_args():
    parser = argparse.ArgumentParser(
        description="Multi-GPU launcher for RL-Probe",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # ä½¿ç”¨GPU 0,1,2
  %(prog)s --gpus 0,1,2

  # ä½¿ç”¨GPU 0,1,2,3ï¼Œæ¯ä¸ªGPUé™åˆ¶30GBæ˜¾å­˜
  %(prog)s --gpus 0,1,2,3 --max-memory 30GiB

  # ä½¿ç”¨æ‰€æœ‰å¯ç”¨GPU
  %(prog)s --gpus all

  # åªè¿è¡Œæ•°æ®å‡†å¤‡å’Œrolloutç”Ÿæˆ
  %(prog)s --gpus 0,1 --steps prepare,rollout

  # æŸ¥çœ‹é…ç½®ä½†ä¸å®é™…è¿è¡Œ
  %(prog)s --gpus 0,1,2 --dry-run
        """,
    )

    # GPUé…ç½®
    parser.add_argument(
        "--gpus",
        type=str,
        required=True,
        help='GPU IDs (é€—å·åˆ†éš”ï¼Œå¦‚ "0,1,2" æˆ– "all" ä½¿ç”¨æ‰€æœ‰GPU)',
    )
    parser.add_argument(
        "--device",
        type=str,
        default="auto",
        choices=["auto", "balanced", "sequential", "cuda"],
        help="è®¾å¤‡åˆ†é…æ¨¡å¼ (é»˜è®¤: auto)",
    )
    parser.add_argument(
        "--max-memory",
        type=str,
        default=None,
        help='æ¯ä¸ªGPUæœ€å¤§æ˜¾å­˜ (å¦‚ "30GiB", é»˜è®¤: ä¸é™åˆ¶)',
    )

    # é…ç½®æ–‡ä»¶
    parser.add_argument(
        "--config",
        type=str,
        default="configs/config.yaml",
        help="é…ç½®æ–‡ä»¶è·¯å¾„ (é»˜è®¤: configs/config.yaml)",
    )
    parser.add_argument(
        "--output-config",
        type=str,
        default=None,
        help="ç”Ÿæˆçš„é…ç½®æ–‡ä»¶ä¿å­˜è·¯å¾„ (é»˜è®¤: configs/config_multi_gpu.yaml)",
    )

    # è¿è¡Œæ­¥éª¤
    parser.add_argument(
        "--steps",
        type=str,
        default="all",
        help='è¿è¡Œçš„æ­¥éª¤ (é€—å·åˆ†éš”: prepare,rollout,kl,visualize æˆ– "all")',
    )

    # å…¶ä»–é€‰é¡¹
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="å¹²è¿è¡Œæ¨¡å¼ï¼šåªæ˜¾ç¤ºé…ç½®ä¸å®é™…è¿è¡Œ",
    )
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="æ˜¾ç¤ºè¯¦ç»†è¾“å‡º",
    )

    return parser.parse_args()


def parse_gpu_ids(gpu_str: str) -> Optional[List[int]]:
    """è§£æGPU IDå­—ç¬¦ä¸²"""
    if gpu_str.lower() == "all":
        return None  # Noneè¡¨ç¤ºä½¿ç”¨æ‰€æœ‰å¯ç”¨GPU

    try:
        gpu_ids = [int(x.strip()) for x in gpu_str.split(",")]
        return gpu_ids
    except ValueError:
        raise ValueError(f"Invalid GPU IDs: {gpu_str}. Use comma-separated integers or 'all'")


def update_config(
    config_path: str,
    gpu_ids: Optional[List[int]],
    device: str,
    max_memory: Optional[str],
    output_path: Optional[str] = None,
) -> str:
    """æ›´æ–°é…ç½®æ–‡ä»¶ä»¥å¯ç”¨å¤šGPU"""

    # è¯»å–åŸå§‹é…ç½®
    with open(config_path) as f:
        config = yaml.safe_load(f)

    # æ›´æ–°ç¡¬ä»¶é…ç½®
    if "hardware" not in config:
        config["hardware"] = {}

    config["hardware"]["device"] = device

    # é…ç½®å¤šGPU
    if "multi_gpu" not in config["hardware"]:
        config["hardware"]["multi_gpu"] = {}

    config["hardware"]["multi_gpu"]["enabled"] = True
    config["hardware"]["multi_gpu"]["gpu_ids"] = gpu_ids

    if max_memory:
        config["hardware"]["multi_gpu"]["max_memory_per_gpu"] = max_memory
    else:
        config["hardware"]["multi_gpu"]["max_memory_per_gpu"] = None

    # ä¿å­˜æ›´æ–°çš„é…ç½®
    if output_path is None:
        output_path = "configs/config_multi_gpu.yaml"

    output_file = Path(output_path)
    output_file.parent.mkdir(parents=True, exist_ok=True)

    with open(output_file, "w") as f:
        yaml.dump(config, f, default_flow_style=False, sort_keys=False)

    return str(output_file)


def print_gpu_info(gpu_ids: Optional[List[int]]):
    """æ‰“å°GPUä¿¡æ¯"""
    logger.info("=" * 60)
    logger.info("GPU é…ç½®ä¿¡æ¯")
    logger.info("=" * 60)

    if not torch.cuda.is_available():
        logger.error("âŒ CUDA ä¸å¯ç”¨ï¼")
        return False

    total_gpus = torch.cuda.device_count()
    logger.info(f"ç³»ç»Ÿæ€»GPUæ•°: {total_gpus}")

    if gpu_ids is None:
        logger.info(f"å°†ä½¿ç”¨æ‰€æœ‰ {total_gpus} ä¸ªGPU")
        gpu_ids_to_show = list(range(total_gpus))
    else:
        logger.info(f"å°†ä½¿ç”¨ {len(gpu_ids)} ä¸ªGPU: {gpu_ids}")
        gpu_ids_to_show = gpu_ids

        # æ£€æŸ¥GPU IDæ˜¯å¦æœ‰æ•ˆ
        for gpu_id in gpu_ids:
            if gpu_id >= total_gpus:
                logger.error(f"âŒ GPU {gpu_id} ä¸å­˜åœ¨ï¼ˆç³»ç»Ÿåªæœ‰ {total_gpus} ä¸ªGPUï¼‰")
                return False

    # æ˜¾ç¤ºæ¯ä¸ªGPUçš„ä¿¡æ¯
    logger.info("\nGPU è¯¦æƒ…:")
    for gpu_id in gpu_ids_to_show:
        props = torch.cuda.get_device_properties(gpu_id)
        memory_gb = props.total_memory / (1024**3)
        logger.info(f"  GPU {gpu_id}: {props.name}")
        logger.info(f"    æ€»æ˜¾å­˜: {memory_gb:.1f} GB")

        # æ˜¾ç¤ºå½“å‰æ˜¾å­˜ä½¿ç”¨æƒ…å†µ
        if torch.cuda.is_initialized():
            allocated = torch.cuda.memory_allocated(gpu_id) / (1024**3)
            cached = torch.cuda.memory_reserved(gpu_id) / (1024**3)
            logger.info(f"    å·²åˆ†é…: {allocated:.2f} GB")
            logger.info(f"    å·²ç¼“å­˜: {cached:.2f} GB")

    logger.info("=" * 60)
    return True


def run_step(script_path: str, config_path: str, step_name: str, verbose: bool = False):
    """è¿è¡Œå•ä¸ªæ­¥éª¤"""
    logger.info(f"\n{'='*60}")
    logger.info(f"æ‰§è¡Œæ­¥éª¤: {step_name}")
    logger.info(f"{'='*60}")

    cmd = [sys.executable, script_path, "--config", config_path]

    # è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œç¡®ä¿å­è¿›ç¨‹èƒ½æ‰¾åˆ° src æ¨¡å—
    env = os.environ.copy()
    project_root = str(Path(__file__).parent.absolute())
    if "PYTHONPATH" in env:
        env["PYTHONPATH"] = f"{project_root}:{env['PYTHONPATH']}"
    else:
        env["PYTHONPATH"] = project_root

    try:
        if verbose:
            result = subprocess.run(cmd, check=True, env=env, cwd=project_root)
        else:
            result = subprocess.run(
                cmd,
                check=True,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                env=env,
                cwd=project_root,
            )
            # åªæ˜¾ç¤ºæœ€åå‡ è¡Œè¾“å‡º
            if result.stdout:
                lines = result.stdout.strip().split("\n")
                for line in lines[-10:]:
                    logger.info(line)

        logger.info(f"âœ… {step_name} å®Œæˆ")
        return True

    except subprocess.CalledProcessError as e:
        logger.error(f"âŒ {step_name} å¤±è´¥")
        if not verbose and e.stderr:
            logger.error(f"é”™è¯¯ä¿¡æ¯:\n{e.stderr}")
        return False


def main():
    args = parse_args()

    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)

    # è§£æGPU IDs
    try:
        gpu_ids = parse_gpu_ids(args.gpus)
    except ValueError as e:
        logger.error(str(e))
        sys.exit(1)

    # æ˜¾ç¤ºGPUä¿¡æ¯
    if not print_gpu_info(gpu_ids):
        sys.exit(1)

    # æ›´æ–°é…ç½®æ–‡ä»¶
    logger.info("\næ›´æ–°é…ç½®æ–‡ä»¶...")
    logger.info(f"  åŸå§‹é…ç½®: {args.config}")

    try:
        new_config_path = update_config(
            config_path=args.config,
            gpu_ids=gpu_ids,
            device=args.device,
            max_memory=args.max_memory,
            output_path=args.output_config,
        )
        logger.info(f"  æ–°é…ç½®ä¿å­˜è‡³: {new_config_path}")
    except Exception as e:
        logger.error(f"âŒ é…ç½®æ›´æ–°å¤±è´¥: {e}")
        sys.exit(1)

    # æ˜¾ç¤ºé…ç½®æ‘˜è¦
    logger.info("\né…ç½®æ‘˜è¦:")
    logger.info(f"  è®¾å¤‡æ¨¡å¼: {args.device}")
    logger.info(f"  GPU IDs: {gpu_ids if gpu_ids else 'æ‰€æœ‰å¯ç”¨GPU'}")
    logger.info(f"  æœ€å¤§æ˜¾å­˜: {args.max_memory if args.max_memory else 'ä¸é™åˆ¶'}")

    # è§£æè¿è¡Œæ­¥éª¤
    if args.steps.lower() == "all":
        steps = ["prepare", "rollout", "kl", "visualize"]
    else:
        steps = [s.strip() for s in args.steps.split(",")]

    logger.info(f"  è¿è¡Œæ­¥éª¤: {', '.join(steps)}")

    if args.dry_run:
        logger.info("\nğŸ” å¹²è¿è¡Œæ¨¡å¼ - ä¸æ‰§è¡Œå®é™…å‘½ä»¤")
        logger.info(f"é…ç½®æ–‡ä»¶å·²ç”Ÿæˆ: {new_config_path}")
        logger.info("\nè¦å®é™…è¿è¡Œï¼Œè¯·ç§»é™¤ --dry-run å‚æ•°")
        return

    # è¿è¡Œå„ä¸ªæ­¥éª¤
    step_scripts = {
        "prepare": "scripts/01_prepare_data.py",
        "rollout": "scripts/02_generate_rollouts.py",
        "kl": "scripts/03_compute_kl.py",
        "visualize": "scripts/04_visualize.py",
    }

    logger.info("\n" + "="*60)
    logger.info("å¼€å§‹æ‰§è¡Œæµç¨‹")
    logger.info("="*60)

    for step in steps:
        if step not in step_scripts:
            logger.warning(f"âš ï¸  æœªçŸ¥æ­¥éª¤: {step}ï¼Œè·³è¿‡")
            continue

        script_path = step_scripts[step]
        if not Path(script_path).exists():
            logger.warning(f"âš ï¸  è„šæœ¬ä¸å­˜åœ¨: {script_path}ï¼Œè·³è¿‡")
            continue

        success = run_step(script_path, new_config_path, step, args.verbose)
        if not success:
            logger.error(f"\nâŒ æ­¥éª¤ '{step}' å¤±è´¥ï¼Œç»ˆæ­¢æ‰§è¡Œ")
            sys.exit(1)

    logger.info("\n" + "="*60)
    logger.info("âœ… æ‰€æœ‰æ­¥éª¤å®Œæˆï¼")
    logger.info("="*60)


if __name__ == "__main__":
    main()
